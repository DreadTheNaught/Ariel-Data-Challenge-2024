{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[273, 271, 284,  ..., 255, 285, 289],\n",
       "        [282, 290, 287,  ..., 304, 300, 296],\n",
       "        [296, 275, 279,  ..., 314, 281, 289],\n",
       "        ...,\n",
       "        [306, 294, 297,  ..., 305, 294, 283],\n",
       "        [284, 289, 294,  ..., 290, 280, 294],\n",
       "        [268, 287, 306,  ..., 251, 280, 273]], dtype=torch.uint16)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "planet_id = 785834\n",
    "f_signal = pd.read_parquet(f'Data/ariel-data-challenge-2024/train/{planet_id}/FGS1_signal.parquet')\n",
    "a_signal = pd.read_parquet(f'Data/ariel-data-challenge-2024/train/{planet_id}/AIRS-CH0_signal.parquet')\n",
    "torch.tensor(f_signal.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\siddh\\Desktop\\deep\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\siddh\\Desktop\\deep\\Ariel-Data-Challenge-2024\\models\\efficientformer_v2.py:692: UserWarning: Overwriting efficientformerv2_s0 in registry with models.efficientformer_v2.efficientformerv2_s0. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def efficientformerv2_s0(pretrained=False, **kwargs) -> EfficientFormerV2:\n",
      "c:\\Users\\siddh\\Desktop\\deep\\Ariel-Data-Challenge-2024\\models\\efficientformer_v2.py:704: UserWarning: Overwriting efficientformerv2_s1 in registry with models.efficientformer_v2.efficientformerv2_s1. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def efficientformerv2_s1(pretrained=False, **kwargs) -> EfficientFormerV2:\n",
      "c:\\Users\\siddh\\Desktop\\deep\\Ariel-Data-Challenge-2024\\models\\efficientformer_v2.py:716: UserWarning: Overwriting efficientformerv2_s2 in registry with models.efficientformer_v2.efficientformerv2_s2. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def efficientformerv2_s2(pretrained=False, **kwargs) -> EfficientFormerV2:\n",
      "c:\\Users\\siddh\\Desktop\\deep\\Ariel-Data-Challenge-2024\\models\\efficientformer_v2.py:728: UserWarning: Overwriting efficientformerv2_l in registry with models.efficientformer_v2.efficientformerv2_l. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def efficientformerv2_l(pretrained=False, **kwargs) -> EfficientFormerV2:\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [16, 1, 3, 3], expected input[1, 3, 224, 224] to have 1 channels, but got 3 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m data_config \u001b[38;5;241m=\u001b[39m timm\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mresolve_model_data_config(model)\n\u001b[0;32m     18\u001b[0m transforms \u001b[38;5;241m=\u001b[39m timm\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcreate_transform(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdata_config, is_training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 20\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransforms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# output is (batch_size, num_features) shaped tensor\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# or equivalently (without needing to set num_classes=0)\u001b[39;00m\n\u001b[0;32m     24\u001b[0m output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward_features(transforms(img)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\siddh\\Desktop\\deep\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\siddh\\Desktop\\deep\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\siddh\\Desktop\\deep\\Ariel-Data-Challenge-2024\\models\\efficientformer_v2.py:650\u001b[0m, in \u001b[0;36mEfficientFormerV2.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    649\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 650\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    651\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_head(x)\n\u001b[0;32m    652\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\siddh\\Desktop\\deep\\Ariel-Data-Challenge-2024\\models\\efficientformer_v2.py:630\u001b[0m, in \u001b[0;36mEfficientFormerV2.forward_features\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_features\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 630\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstages(x)\n\u001b[0;32m    632\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n",
      "File \u001b[1;32mc:\\Users\\siddh\\Desktop\\deep\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\siddh\\Desktop\\deep\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\siddh\\Desktop\\deep\\.venv\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\siddh\\Desktop\\deep\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\siddh\\Desktop\\deep\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\siddh\\Desktop\\deep\\.venv\\lib\\site-packages\\timm\\layers\\conv_bn_act.py:83\u001b[0m, in \u001b[0;36mConvNormAct.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 83\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn(x)\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maa \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\siddh\\Desktop\\deep\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\siddh\\Desktop\\deep\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\siddh\\Desktop\\deep\\.venv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\siddh\\Desktop\\deep\\.venv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [16, 1, 3, 3], expected input[1, 3, 224, 224] to have 1 channels, but got 3 channels instead"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from PIL import Image\n",
    "import timm\n",
    "import models.efficientformer_v2\n",
    "\n",
    "img = Image.open(\n",
    "    urlopen('https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'))\n",
    "\n",
    "model = timm.create_model(\n",
    "    'efficientformerv2_s0.snap_dist_in1k',\n",
    "    pretrained=True,\n",
    "    num_classes=0,  # remove classifier nn.Linear\n",
    ")\n",
    "model = model.eval()\n",
    "\n",
    "data_config = timm.data.resolve_model_data_config(model)\n",
    "transforms = timm.data.create_transform(**data_config, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EfficientFormerV2(\n",
       "  (stem): Stem4(\n",
       "    (conv1): ConvNormAct(\n",
       "      (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (bn): BatchNormAct2d(\n",
       "        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "        (drop): Identity()\n",
       "        (act): GELU()\n",
       "      )\n",
       "    )\n",
       "    (conv2): ConvNormAct(\n",
       "      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (bn): BatchNormAct2d(\n",
       "        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "        (drop): Identity()\n",
       "        (act): GELU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (stages): Sequential(\n",
       "    (0): EfficientFormerV2Stage(\n",
       "      (downsample): Identity()\n",
       "      (blocks): Sequential(\n",
       "        (0): EfficientFormerV2Block(\n",
       "          (mlp): ConvMlpWithNorm(\n",
       "            (fc1): ConvNormAct(\n",
       "              (conv): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): BatchNormAct2d(\n",
       "                128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): GELU()\n",
       "              )\n",
       "            )\n",
       "            (mid): ConvNormAct(\n",
       "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "              (bn): BatchNormAct2d(\n",
       "                128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): GELU()\n",
       "              )\n",
       "            )\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (fc2): ConvNorm(\n",
       "              (conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): LayerScale2d()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (1): EfficientFormerV2Block(\n",
       "          (mlp): ConvMlpWithNorm(\n",
       "            (fc1): ConvNormAct(\n",
       "              (conv): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): BatchNormAct2d(\n",
       "                128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): GELU()\n",
       "              )\n",
       "            )\n",
       "            (mid): ConvNormAct(\n",
       "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "              (bn): BatchNormAct2d(\n",
       "                128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): GELU()\n",
       "              )\n",
       "            )\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (fc2): ConvNorm(\n",
       "              (conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): LayerScale2d()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): EfficientFormerV2Stage(\n",
       "      (downsample): Downsample(\n",
       "        (conv): ConvNorm(\n",
       "          (conv): Conv2d(32, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): EfficientFormerV2Block(\n",
       "          (mlp): ConvMlpWithNorm(\n",
       "            (fc1): ConvNormAct(\n",
       "              (conv): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): BatchNormAct2d(\n",
       "                192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): GELU()\n",
       "              )\n",
       "            )\n",
       "            (mid): ConvNormAct(\n",
       "              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)\n",
       "              (bn): BatchNormAct2d(\n",
       "                192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): GELU()\n",
       "              )\n",
       "            )\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (fc2): ConvNorm(\n",
       "              (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): LayerScale2d()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (1): EfficientFormerV2Block(\n",
       "          (mlp): ConvMlpWithNorm(\n",
       "            (fc1): ConvNormAct(\n",
       "              (conv): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): BatchNormAct2d(\n",
       "                192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): GELU()\n",
       "              )\n",
       "            )\n",
       "            (mid): ConvNormAct(\n",
       "              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)\n",
       "              (bn): BatchNormAct2d(\n",
       "                192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): GELU()\n",
       "              )\n",
       "            )\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (fc2): ConvNorm(\n",
       "              (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): LayerScale2d()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): EfficientFormerV2Stage(\n",
       "      (downsample): Downsample(\n",
       "        (conv): ConvNorm(\n",
       "          (conv): Conv2d(48, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): EfficientFormerV2Block(\n",
       "          (mlp): ConvMlpWithNorm(\n",
       "            (fc1): ConvNormAct(\n",
       "              (conv): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): BatchNormAct2d(\n",
       "                384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): GELU()\n",
       "              )\n",
       "            )\n",
       "            (mid): ConvNormAct(\n",
       "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "              (bn): BatchNormAct2d(\n",
       "                384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): GELU()\n",
       "              )\n",
       "            )\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (fc2): ConvNorm(\n",
       "              (conv): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): LayerScale2d()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (1): EfficientFormerV2Block(\n",
       "          (mlp): ConvMlpWithNorm(\n",
       "            (fc1): ConvNormAct(\n",
       "              (conv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): BatchNormAct2d(\n",
       "                288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): GELU()\n",
       "              )\n",
       "            )\n",
       "            (mid): ConvNormAct(\n",
       "              (conv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288)\n",
       "              (bn): BatchNormAct2d(\n",
       "                288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): GELU()\n",
       "              )\n",
       "            )\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (fc2): ConvNorm(\n",
       "              (conv): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): LayerScale2d()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (2): EfficientFormerV2Block(\n",
       "          (mlp): ConvMlpWithNorm(\n",
       "            (fc1): ConvNormAct(\n",
       "              (conv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): BatchNormAct2d(\n",
       "                288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): GELU()\n",
       "              )\n",
       "            )\n",
       "            (mid): ConvNormAct(\n",
       "              (conv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288)\n",
       "              (bn): BatchNormAct2d(\n",
       "                288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): GELU()\n",
       "              )\n",
       "            )\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (fc2): ConvNorm(\n",
       "              (conv): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): LayerScale2d()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (3): EfficientFormerV2Block(\n",
       "          (mlp): ConvMlpWithNorm(\n",
       "            (fc1): ConvNormAct(\n",
       "              (conv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): BatchNormAct2d(\n",
       "                288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): GELU()\n",
       "              )\n",
       "            )\n",
       "            (mid): ConvNormAct(\n",
       "              (conv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288)\n",
       "              (bn): BatchNormAct2d(\n",
       "                288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): GELU()\n",
       "              )\n",
       "            )\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (fc2): ConvNorm(\n",
       "              (conv): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): LayerScale2d()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (4): EfficientFormerV2Block(\n",
       "          (token_mixer): Attention2d(\n",
       "            (stride_conv): ConvNorm(\n",
       "              (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96)\n",
       "              (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (upsample): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "            (q): ConvNorm(\n",
       "              (conv): Conv2d(96, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (k): ConvNorm(\n",
       "              (conv): Conv2d(96, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (v): ConvNorm(\n",
       "              (conv): Conv2d(96, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (v_local): ConvNorm(\n",
       "              (conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "              (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (talking_head1): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (talking_head2): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): GELU()\n",
       "            (proj): ConvNorm(\n",
       "              (conv): Conv2d(1024, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (ls1): LayerScale2d()\n",
       "          (drop_path1): Identity()\n",
       "          (mlp): ConvMlpWithNorm(\n",
       "            (fc1): ConvNormAct(\n",
       "              (conv): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): BatchNormAct2d(\n",
       "                384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): GELU()\n",
       "              )\n",
       "            )\n",
       "            (mid): ConvNormAct(\n",
       "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "              (bn): BatchNormAct2d(\n",
       "                384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): GELU()\n",
       "              )\n",
       "            )\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (fc2): ConvNorm(\n",
       "              (conv): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): LayerScale2d()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (5): EfficientFormerV2Block(\n",
       "          (token_mixer): Attention2d(\n",
       "            (stride_conv): ConvNorm(\n",
       "              (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96)\n",
       "              (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (upsample): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "            (q): ConvNorm(\n",
       "              (conv): Conv2d(96, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (k): ConvNorm(\n",
       "              (conv): Conv2d(96, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (v): ConvNorm(\n",
       "              (conv): Conv2d(96, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (v_local): ConvNorm(\n",
       "              (conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "              (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (talking_head1): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (talking_head2): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): GELU()\n",
       "            (proj): ConvNorm(\n",
       "              (conv): Conv2d(1024, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (ls1): LayerScale2d()\n",
       "          (drop_path1): Identity()\n",
       "          (mlp): ConvMlpWithNorm(\n",
       "            (fc1): ConvNormAct(\n",
       "              (conv): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): BatchNormAct2d(\n",
       "                384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): GELU()\n",
       "              )\n",
       "            )\n",
       "            (mid): ConvNormAct(\n",
       "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "              (bn): BatchNormAct2d(\n",
       "                384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): GELU()\n",
       "              )\n",
       "            )\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (fc2): ConvNorm(\n",
       "              (conv): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): LayerScale2d()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): EfficientFormerV2Stage(\n",
       "      (downsample): Downsample(\n",
       "        (conv): ConvNorm(\n",
       "          (conv): Conv2d(96, 176, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(176, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (attn): Attention2dDownsample(\n",
       "          (q): LocalGlobalQuery(\n",
       "            (pool): AvgPool2d(kernel_size=1, stride=2, padding=0)\n",
       "            (local): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96)\n",
       "            (proj): ConvNorm(\n",
       "              (conv): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (k): ConvNorm(\n",
       "            (conv): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (v): ConvNorm(\n",
       "            (conv): Conv2d(96, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (v_local): ConvNorm(\n",
       "            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512)\n",
       "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (act): GELU()\n",
       "          (proj): ConvNorm(\n",
       "            (conv): Conv2d(512, 176, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (bn): BatchNorm2d(176, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): EfficientFormerV2Block(\n",
       "          (mlp): ConvMlpWithNorm(\n",
       "            (fc1): ConvNormAct(\n",
       "              (conv): Conv2d(176, 704, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): BatchNormAct2d(\n",
       "                704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): GELU()\n",
       "              )\n",
       "            )\n",
       "            (mid): ConvNormAct(\n",
       "              (conv): Conv2d(704, 704, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=704)\n",
       "              (bn): BatchNormAct2d(\n",
       "                704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): GELU()\n",
       "              )\n",
       "            )\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (fc2): ConvNorm(\n",
       "              (conv): Conv2d(704, 176, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): BatchNorm2d(176, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): LayerScale2d()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (1): EfficientFormerV2Block(\n",
       "          (mlp): ConvMlpWithNorm(\n",
       "            (fc1): ConvNormAct(\n",
       "              (conv): Conv2d(176, 528, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): BatchNormAct2d(\n",
       "                528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): GELU()\n",
       "              )\n",
       "            )\n",
       "            (mid): ConvNormAct(\n",
       "              (conv): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528)\n",
       "              (bn): BatchNormAct2d(\n",
       "                528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): GELU()\n",
       "              )\n",
       "            )\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (fc2): ConvNorm(\n",
       "              (conv): Conv2d(528, 176, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): BatchNorm2d(176, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): LayerScale2d()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (2): EfficientFormerV2Block(\n",
       "          (token_mixer): Attention2d(\n",
       "            (q): ConvNorm(\n",
       "              (conv): Conv2d(176, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (k): ConvNorm(\n",
       "              (conv): Conv2d(176, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (v): ConvNorm(\n",
       "              (conv): Conv2d(176, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (v_local): ConvNorm(\n",
       "              (conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "              (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (talking_head1): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (talking_head2): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): GELU()\n",
       "            (proj): ConvNorm(\n",
       "              (conv): Conv2d(1024, 176, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): BatchNorm2d(176, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (ls1): LayerScale2d()\n",
       "          (drop_path1): Identity()\n",
       "          (mlp): ConvMlpWithNorm(\n",
       "            (fc1): ConvNormAct(\n",
       "              (conv): Conv2d(176, 528, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): BatchNormAct2d(\n",
       "                528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): GELU()\n",
       "              )\n",
       "            )\n",
       "            (mid): ConvNormAct(\n",
       "              (conv): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528)\n",
       "              (bn): BatchNormAct2d(\n",
       "                528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): GELU()\n",
       "              )\n",
       "            )\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (fc2): ConvNorm(\n",
       "              (conv): Conv2d(528, 176, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): BatchNorm2d(176, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): LayerScale2d()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (3): EfficientFormerV2Block(\n",
       "          (token_mixer): Attention2d(\n",
       "            (q): ConvNorm(\n",
       "              (conv): Conv2d(176, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (k): ConvNorm(\n",
       "              (conv): Conv2d(176, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (v): ConvNorm(\n",
       "              (conv): Conv2d(176, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (v_local): ConvNorm(\n",
       "              (conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "              (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (talking_head1): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (talking_head2): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): GELU()\n",
       "            (proj): ConvNorm(\n",
       "              (conv): Conv2d(1024, 176, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): BatchNorm2d(176, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (ls1): LayerScale2d()\n",
       "          (drop_path1): Identity()\n",
       "          (mlp): ConvMlpWithNorm(\n",
       "            (fc1): ConvNormAct(\n",
       "              (conv): Conv2d(176, 704, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): BatchNormAct2d(\n",
       "                704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): GELU()\n",
       "              )\n",
       "            )\n",
       "            (mid): ConvNormAct(\n",
       "              (conv): Conv2d(704, 704, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=704)\n",
       "              (bn): BatchNormAct2d(\n",
       "                704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): GELU()\n",
       "              )\n",
       "            )\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (fc2): ConvNorm(\n",
       "              (conv): Conv2d(704, 176, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): BatchNorm2d(176, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): LayerScale2d()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): BatchNorm2d(176, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (head_drop): Dropout(p=0.0, inplace=False)\n",
       "  (head): Identity()\n",
       "  (head_dist): Identity()\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FGS1 shape (135000, 1024)\n",
      "AIRS-CH0 shape (11250, 11392)\n"
     ]
    }
   ],
   "source": [
    "print(f'FGS1 shape {f_signal.to_numpy().shape}\\nAIRS-CH0 shape {a_signal.to_numpy().shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_parquet(f'Data/ariel-data-challenge-2024/train/{planet_id}/FGS1_calibration/dark.parquet').to_numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106.06601717798213"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "math.sqrt(11250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "_, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "sns.heatmap(f_signal.iloc[0].values.reshape(32, 32), ax=ax1, vmin=0, vmax=52000)\n",
    "ax1.set_aspect('equal')\n",
    "sns.heatmap(f_signal.iloc[1].values.reshape(32, 32), ax=ax2, vmin=0, vmax=52000)\n",
    "ax2.set_aspect('equal')\n",
    "plt.suptitle('A pair of FGS1 images')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[701, 730, 709,  ..., 738, 723, 710],\n",
       "         [710, 712, 711,  ..., 718, 715, 735],\n",
       "         [712, 721, 695,  ..., 708, 720, 724],\n",
       "         ...,\n",
       "         [711, 702, 726,  ..., 729, 728, 719],\n",
       "         [739, 717, 710,  ..., 732, 705, 725],\n",
       "         [726, 730, 717,  ..., 727, 725, 714]], dtype=torch.uint16),\n",
       " tensor([[273, 271, 284,  ..., 255, 285, 289],\n",
       "         [282, 290, 287,  ..., 304, 300, 296],\n",
       "         [296, 275, 279,  ..., 314, 281, 289],\n",
       "         ...,\n",
       "         [306, 294, 297,  ..., 305, 294, 283],\n",
       "         [284, 289, 294,  ..., 290, 280, 294],\n",
       "         [268, 287, 306,  ..., 251, 280, 273]], dtype=torch.uint16),\n",
       " 0,\n",
       " 'D:/Data/train\\\\785834\\\\AIRS-CH0_signal.parquet')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import dataloader\n",
    "\n",
    "train_data = dataloader.CustomDataset(\"D:/Data/train\", \"D:/Data/train_labels.csv\")\n",
    "train_loader = DataLoader(train_data, batch_size= 1)\n",
    "\n",
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[1313, 1282, 1248,  ..., 1252, 1272, 1304],\n",
      "         [1277, 1305, 1334,  ..., 1299, 1291, 1280],\n",
      "         [1290, 1268, 1279,  ..., 1274, 1262, 1291],\n",
      "         ...,\n",
      "         [1291, 1282, 1308,  ..., 1287, 1307, 1297],\n",
      "         [1289, 1300, 1272,  ..., 1295, 1299, 1318],\n",
      "         [1303, 1269, 1262,  ..., 1321, 1280, 1298]]], dtype=torch.uint16), tensor([[[359, 335, 346,  ..., 354, 375, 355],\n",
      "         [339, 352, 367,  ..., 355, 354, 365],\n",
      "         [338, 342, 344,  ..., 336, 334, 374],\n",
      "         ...,\n",
      "         [363, 338, 324,  ..., 260, 345, 348],\n",
      "         [360, 358, 316,  ..., 366, 338, 351],\n",
      "         [374, 326, 387,  ..., 327, 345, 335]]], dtype=torch.uint16), tensor([0])]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import dataloader\n",
    "\n",
    "train_data = dataloader.CustomDataset(\"D:/Data/train\", \"D:/Data/train_labels.csv\")\n",
    "train_loader = DataLoader(train_data, batch_size= 1)\n",
    "\n",
    "for data in train_loader:\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\siddh\\Desktop\\deep\\Ariel-Data-Challenge-2024\\Data\\ariel-data-challenge-2024\\train_adc_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>planet_id</th>\n",
       "      <th>FGS1_adc_offset</th>\n",
       "      <th>FGS1_adc_gain</th>\n",
       "      <th>AIRS-CH0_adc_offset</th>\n",
       "      <th>AIRS-CH0_adc_gain</th>\n",
       "      <th>star</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>785834</td>\n",
       "      <td>-343.335938</td>\n",
       "      <td>0.837244</td>\n",
       "      <td>-778.916533</td>\n",
       "      <td>0.924746</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   planet_id  FGS1_adc_offset  FGS1_adc_gain  AIRS-CH0_adc_offset  \\\n",
       "0     785834      -343.335938       0.837244          -778.916533   \n",
       "\n",
       "   AIRS-CH0_adc_gain  star  \n",
       "0           0.924746     1  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['planet_id'] == planet_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from natsort import natsorted\n",
    "import numpy as np\n",
    "import preprocessing.utils\n",
    "import pandas as pd\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root, flag, split, a_transform=None, f_transform=None):\n",
    "        super().__init__()\n",
    "        self.f_data = []\n",
    "        self.a_data = []\n",
    "        self.a_transform = a_transform\n",
    "        self.f_transform = f_transform\n",
    "        self.root_dir = root\n",
    "        self.cut_inf, self.cut_sup = 39, 321\n",
    "        self.files = natsorted(os.listdir(os.path.join(self.root_dir, split)))\n",
    "        self.adc = pd.read_csv(os.path.join(root, f'{split}_adc_info.csv'))\n",
    "        axis = pd.read_parquet(os.path.join(self.root_dir, 'axis_info.parquet'))\n",
    "        self.integration_airs = axis['AIRS-CH0-integration_time'].dropna().values\n",
    "        self.integration_fgs1 = np.ones(axis.shape[0]) * 0.1\n",
    "        self.flag = flag\n",
    "        self.split = split\n",
    "\n",
    "        if self.split == \"train\":\n",
    "            self.labels = pd.read_csv(os.path.join(root, f'{split}_labels.csv'))\n",
    "\n",
    "        for planet in self.files:\n",
    "            self.a_data.append((os.path.join(self.root_dir, split, planet, \"AIRS-CH0_signal.parquet\"), int(planet)))\n",
    "            self.f_data.append((os.path.join(self.root_dir, split, planet, \"FGS1_signal.parquet\"), int(planet)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.a_data)  # Assuming both AIRS and FGS1 data have the same length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        AIRS_data_path, a_planet = self.a_data[index]\n",
    "        FGS1_data_path, f_planet = self.f_data[index]\n",
    "\n",
    "        AIRS_data = pd.read_parquet(AIRS_data_path)\n",
    "        FGS1_data = pd.read_parquet(FGS1_data_path)\n",
    "        \n",
    "        AIRS_data = AIRS_data.values.astype(np.float64).reshape((AIRS_data.shape[0], 32, 356))\n",
    "        FGS1_data = FGS1_data.values.astype(np.float64).reshape((FGS1_data.shape[0], 32, 32))\n",
    "\n",
    "        adc_info = self.adc.loc[self.adc['planet_id'] == a_planet].iloc[0]\n",
    "        print('before ACD')\n",
    "        AIRS_data = preprocessing.utils.adc_convert(\n",
    "            AIRS_data,\n",
    "            float(adc_info['AIRS-CH0_adc_gain']),\n",
    "            float(adc_info['AIRS-CH0_adc_offset'])\n",
    "        )\n",
    "        FGS1_data = preprocessing.utils.adc_convert(\n",
    "            FGS1_data,\n",
    "            float(adc_info['FGS1_adc_gain']),\n",
    "            float(adc_info['FGS1_adc_offset'])\n",
    "        )\n",
    "        print('AFTER ACD')\n",
    "\n",
    "        AIRS_data = AIRS_data[:, :, self.cut_inf:self.cut_sup]\n",
    "\n",
    "        calibration_files = {\n",
    "            'flat': 'flat.parquet',\n",
    "            'dark': 'dark.parquet',\n",
    "            'dead': 'dead.parquet',\n",
    "            'linear_corr': 'linear_corr.parquet'\n",
    "        }\n",
    "\n",
    "        def load_calibration_data(planet_id, calibration_type, cut_range=None):\n",
    "            path = os.path.join(self.root_dir, f'{self.split}/{planet_id}/AIRS-CH0_calibration/{calibration_files[calibration_type]}')\n",
    "            data = pd.read_parquet(path).values.astype(np.float64)\n",
    "            if cut_range:\n",
    "                data = data[:, cut_range[0]:cut_range[1]]\n",
    "            return data\n",
    "\n",
    "        flat_airs = load_calibration_data(a_planet, 'flat', (self.cut_inf, self.cut_sup))\n",
    "        dark_airs = load_calibration_data(a_planet, 'dark', (self.cut_inf, self.cut_sup))\n",
    "        dead_airs = load_calibration_data(a_planet, 'dead', (self.cut_inf, self.cut_sup))\n",
    "        linear_corr_airs = load_calibration_data(a_planet, 'linear_corr', (self.cut_inf, self.cut_sup))\n",
    "\n",
    "        flat_fgs1 = load_calibration_data(f_planet, 'flat')\n",
    "        dark_fgs1 = load_calibration_data(f_planet, 'dark')\n",
    "        dead_fgs1 = load_calibration_data(f_planet, 'dead')\n",
    "        linear_corr_fgs1 = load_calibration_data(f_planet, 'linear_corr')\n",
    "\n",
    "        if self.flag['MASK']:\n",
    "            print('are We going in ?')\n",
    "            AIRS_data = preprocessing.utils.mask_hot_dead(AIRS_data, dead_airs, dark_airs)\n",
    "            FGS1_data = preprocessing.utils.mask_hot_dead(FGS1_data, dead_fgs1, dark_fgs1)\n",
    "            print('applied mask')\n",
    "        if self.flag[\"NLCORR\"]:\n",
    "            AIRS_data = preprocessing.utils.apply_linear_corr(linear_corr_airs, AIRS_data)\n",
    "            FGS1_data = preprocessing.utils.apply_linear_corr(linear_corr_fgs1, FGS1_data)\n",
    "            print('applied linear correction')\n",
    "\n",
    "        if self.flag[\"DARK\"]:\n",
    "            AIRS_data = preprocessing.utils.clean_dark(AIRS_data, dead_airs, dark_airs, self.integration_airs)\n",
    "            FGS1_data = preprocessing.utils.clean_dark(FGS1_data, dead_fgs1, dark_fgs1, self.integration_fgs1)\n",
    "            print('corrected dark current')\n",
    "\n",
    "        AIRS_data = preprocessing.utils.get_cds(AIRS_data)\n",
    "        FGS1_data = preprocessing.utils.get_cds(FGS1_data)\n",
    "\n",
    "        if self.a_transform:\n",
    "            AIRS_data = self.a_transform(AIRS_data)\n",
    "        if self.f_transform:\n",
    "            FGS1_data = self.f_transform(FGS1_data)\n",
    "\n",
    "        if self.flag[\"TIME_BINNING\"]:\n",
    "            AIRS_data = preprocessing.utils.bin_obs(AIRS_data, binning=30)\n",
    "            FGS1_data = preprocessing.utils.bin_obs(FGS1_data, binning=30 * 12)\n",
    "            print('done binning')\n",
    "        else:\n",
    "            AIRS_data = AIRS_data.transpose(0, 2, 1)\n",
    "            FGS1_data = FGS1_data.transpose(0, 2, 1)\n",
    "\n",
    "        if self.flag[\"FLAT\"]:\n",
    "            AIRS_data = preprocessing.utils.correct_flat_field(flat_airs, dead_airs, AIRS_data)\n",
    "            FGS1_data = preprocessing.utils.correct_flat_field(flat_fgs1, dead_fgs1, FGS1_data)\n",
    "            print('flat')\n",
    "\n",
    "        if self.split == \"train\":\n",
    "            label = self.labels[self.labels['planet_id'] == a_planet]['value'].values[0]\n",
    "            data = (AIRS_data, FGS1_data, label)\n",
    "            print('made data')\n",
    "        else:  # For test split\n",
    "            data = (AIRS_data, FGS1_data)\n",
    "\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.load(r'C:\\Users\\siddh\\Desktop\\deep\\Ariel-Data-Challenge-2024\\preprocessed\\785834\\FGS1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([187, 32, 32])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "673"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "len(os.listdir(r'C:\\Users\\siddh\\Desktop\\deep\\Ariel-Data-Challenge-2024\\preprocessed'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CustomDataset(r'Data/ariel-data-challenge-2024',{'MASK':True,'NLCORR':True,'DARK':True,'TIME_BINNING':True,'FLAT':True},split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before ACD\n",
      "AFTER ACD\n",
      "are We going in ?\n"
     ]
    }
   ],
   "source": [
    "for i,j,k in data:\n",
    "    print(i.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'i' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mi\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;31mNameError\u001b[0m: name 'i' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AIRS-CH0-axis0-h</th>\n",
       "      <th>AIRS-CH0-axis2-um</th>\n",
       "      <th>AIRS-CH0-integration_time</th>\n",
       "      <th>FGS1-axis0-h</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000028</td>\n",
       "      <td>4.078463</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000056</td>\n",
       "      <td>4.074023</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.000056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001361</td>\n",
       "      <td>4.069568</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001389</td>\n",
       "      <td>4.065100</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.000167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.002694</td>\n",
       "      <td>4.060618</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.499722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.499806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.499833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.499917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.499944</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>135000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        AIRS-CH0-axis0-h  AIRS-CH0-axis2-um  AIRS-CH0-integration_time  \\\n",
       "0               0.000028           4.078463                        0.1   \n",
       "1               0.000056           4.074023                        4.5   \n",
       "2               0.001361           4.069568                        0.1   \n",
       "3               0.001389           4.065100                        4.5   \n",
       "4               0.002694           4.060618                        0.1   \n",
       "...                  ...                ...                        ...   \n",
       "134995               NaN                NaN                        NaN   \n",
       "134996               NaN                NaN                        NaN   \n",
       "134997               NaN                NaN                        NaN   \n",
       "134998               NaN                NaN                        NaN   \n",
       "134999               NaN                NaN                        NaN   \n",
       "\n",
       "        FGS1-axis0-h  \n",
       "0           0.000028  \n",
       "1           0.000056  \n",
       "2           0.000139  \n",
       "3           0.000167  \n",
       "4           0.000250  \n",
       "...              ...  \n",
       "134995      7.499722  \n",
       "134996      7.499806  \n",
       "134997      7.499833  \n",
       "134998      7.499917  \n",
       "134999      7.499944  \n",
       "\n",
       "[135000 rows x 4 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_parquet(r'C:\\Users\\siddh\\Desktop\\deep\\Ariel-Data-Challenge-2024\\Data\\ariel-data-challenge-2024\\axis_info.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(135000, 1024)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "planet_id = 14485303\n",
    "f_signal = pd.read_parquet(f'data/ariel-data-challenge-2024/train/{planet_id}/FGS1_signal.parquet')\n",
    "f_signal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_signal = f_signal.values.mean(axis=1)\n",
    "net_signal = mean_signal[1::2] - mean_signal[0::2]\n",
    "cum_signal = net_signal.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_0</th>\n",
       "      <th>column_1</th>\n",
       "      <th>column_2</th>\n",
       "      <th>column_3</th>\n",
       "      <th>column_4</th>\n",
       "      <th>column_5</th>\n",
       "      <th>column_6</th>\n",
       "      <th>column_7</th>\n",
       "      <th>column_8</th>\n",
       "      <th>column_9</th>\n",
       "      <th>...</th>\n",
       "      <th>column_22</th>\n",
       "      <th>column_23</th>\n",
       "      <th>column_24</th>\n",
       "      <th>column_25</th>\n",
       "      <th>column_26</th>\n",
       "      <th>column_27</th>\n",
       "      <th>column_28</th>\n",
       "      <th>column_29</th>\n",
       "      <th>column_30</th>\n",
       "      <th>column_31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13.447132</td>\n",
       "      <td>13.037364</td>\n",
       "      <td>13.320732</td>\n",
       "      <td>13.934092</td>\n",
       "      <td>14.160968</td>\n",
       "      <td>12.997231</td>\n",
       "      <td>13.605174</td>\n",
       "      <td>13.179105</td>\n",
       "      <td>13.031221</td>\n",
       "      <td>13.514571</td>\n",
       "      <td>...</td>\n",
       "      <td>12.830434</td>\n",
       "      <td>13.167031</td>\n",
       "      <td>12.889489</td>\n",
       "      <td>13.446304</td>\n",
       "      <td>13.519172</td>\n",
       "      <td>13.556801</td>\n",
       "      <td>13.398178</td>\n",
       "      <td>12.817820</td>\n",
       "      <td>12.855377</td>\n",
       "      <td>13.383958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.799192</td>\n",
       "      <td>15.720345</td>\n",
       "      <td>12.893337</td>\n",
       "      <td>13.209815</td>\n",
       "      <td>13.018760</td>\n",
       "      <td>12.853811</td>\n",
       "      <td>14.092373</td>\n",
       "      <td>28.917124</td>\n",
       "      <td>14.327434</td>\n",
       "      <td>12.952086</td>\n",
       "      <td>...</td>\n",
       "      <td>13.051660</td>\n",
       "      <td>13.315811</td>\n",
       "      <td>12.645670</td>\n",
       "      <td>13.236526</td>\n",
       "      <td>12.655067</td>\n",
       "      <td>13.625855</td>\n",
       "      <td>15.532673</td>\n",
       "      <td>13.316868</td>\n",
       "      <td>13.065052</td>\n",
       "      <td>13.007496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14.011855</td>\n",
       "      <td>14.344862</td>\n",
       "      <td>13.188338</td>\n",
       "      <td>14.408124</td>\n",
       "      <td>13.020654</td>\n",
       "      <td>13.505918</td>\n",
       "      <td>12.732587</td>\n",
       "      <td>12.829406</td>\n",
       "      <td>12.820350</td>\n",
       "      <td>13.804098</td>\n",
       "      <td>...</td>\n",
       "      <td>13.279897</td>\n",
       "      <td>13.659994</td>\n",
       "      <td>12.731043</td>\n",
       "      <td>14.440821</td>\n",
       "      <td>13.113338</td>\n",
       "      <td>12.870840</td>\n",
       "      <td>13.968151</td>\n",
       "      <td>15.748286</td>\n",
       "      <td>13.902305</td>\n",
       "      <td>12.735503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.223301</td>\n",
       "      <td>13.600391</td>\n",
       "      <td>12.934800</td>\n",
       "      <td>17.042988</td>\n",
       "      <td>15.969407</td>\n",
       "      <td>46.141069</td>\n",
       "      <td>13.909606</td>\n",
       "      <td>13.255826</td>\n",
       "      <td>14.416834</td>\n",
       "      <td>12.676454</td>\n",
       "      <td>...</td>\n",
       "      <td>12.976551</td>\n",
       "      <td>12.928398</td>\n",
       "      <td>13.247288</td>\n",
       "      <td>13.112748</td>\n",
       "      <td>13.768122</td>\n",
       "      <td>13.077388</td>\n",
       "      <td>13.206464</td>\n",
       "      <td>12.941768</td>\n",
       "      <td>13.240524</td>\n",
       "      <td>12.603768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.462461</td>\n",
       "      <td>32.868870</td>\n",
       "      <td>13.565033</td>\n",
       "      <td>12.948105</td>\n",
       "      <td>13.117695</td>\n",
       "      <td>13.190062</td>\n",
       "      <td>13.625904</td>\n",
       "      <td>12.907982</td>\n",
       "      <td>14.176126</td>\n",
       "      <td>14.645302</td>\n",
       "      <td>...</td>\n",
       "      <td>13.271953</td>\n",
       "      <td>13.048486</td>\n",
       "      <td>14.321457</td>\n",
       "      <td>17.067648</td>\n",
       "      <td>13.212197</td>\n",
       "      <td>13.434782</td>\n",
       "      <td>13.049322</td>\n",
       "      <td>13.067504</td>\n",
       "      <td>12.892875</td>\n",
       "      <td>12.835842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>13.029042</td>\n",
       "      <td>20.261400</td>\n",
       "      <td>13.222501</td>\n",
       "      <td>13.414526</td>\n",
       "      <td>13.265013</td>\n",
       "      <td>13.911818</td>\n",
       "      <td>13.364908</td>\n",
       "      <td>13.694575</td>\n",
       "      <td>12.849393</td>\n",
       "      <td>12.926298</td>\n",
       "      <td>...</td>\n",
       "      <td>13.101895</td>\n",
       "      <td>14.444280</td>\n",
       "      <td>12.707569</td>\n",
       "      <td>15.358972</td>\n",
       "      <td>18.928262</td>\n",
       "      <td>13.020566</td>\n",
       "      <td>13.191545</td>\n",
       "      <td>13.343338</td>\n",
       "      <td>13.207074</td>\n",
       "      <td>12.902596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>13.288539</td>\n",
       "      <td>17.563226</td>\n",
       "      <td>13.203073</td>\n",
       "      <td>14.250993</td>\n",
       "      <td>16.465606</td>\n",
       "      <td>13.475710</td>\n",
       "      <td>13.174175</td>\n",
       "      <td>13.166318</td>\n",
       "      <td>13.590350</td>\n",
       "      <td>13.119641</td>\n",
       "      <td>...</td>\n",
       "      <td>12.936764</td>\n",
       "      <td>17.106738</td>\n",
       "      <td>13.071521</td>\n",
       "      <td>13.183591</td>\n",
       "      <td>13.152070</td>\n",
       "      <td>15.829402</td>\n",
       "      <td>12.605497</td>\n",
       "      <td>16.573056</td>\n",
       "      <td>13.396213</td>\n",
       "      <td>13.353961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13.698956</td>\n",
       "      <td>12.809229</td>\n",
       "      <td>13.616637</td>\n",
       "      <td>13.576106</td>\n",
       "      <td>13.425336</td>\n",
       "      <td>12.761027</td>\n",
       "      <td>14.480694</td>\n",
       "      <td>13.610478</td>\n",
       "      <td>13.357413</td>\n",
       "      <td>13.475321</td>\n",
       "      <td>...</td>\n",
       "      <td>14.503129</td>\n",
       "      <td>12.979057</td>\n",
       "      <td>13.687640</td>\n",
       "      <td>13.118037</td>\n",
       "      <td>33.735586</td>\n",
       "      <td>13.335105</td>\n",
       "      <td>13.800307</td>\n",
       "      <td>13.748396</td>\n",
       "      <td>15.201048</td>\n",
       "      <td>13.346287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14.219683</td>\n",
       "      <td>13.933556</td>\n",
       "      <td>13.126639</td>\n",
       "      <td>13.369487</td>\n",
       "      <td>13.163664</td>\n",
       "      <td>12.818119</td>\n",
       "      <td>13.740463</td>\n",
       "      <td>12.767789</td>\n",
       "      <td>32.232798</td>\n",
       "      <td>12.942297</td>\n",
       "      <td>...</td>\n",
       "      <td>13.261890</td>\n",
       "      <td>12.887265</td>\n",
       "      <td>13.089404</td>\n",
       "      <td>12.941111</td>\n",
       "      <td>13.672106</td>\n",
       "      <td>13.989907</td>\n",
       "      <td>13.435668</td>\n",
       "      <td>14.519015</td>\n",
       "      <td>13.414396</td>\n",
       "      <td>14.830492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>17.730966</td>\n",
       "      <td>14.513986</td>\n",
       "      <td>13.172437</td>\n",
       "      <td>13.306472</td>\n",
       "      <td>13.168511</td>\n",
       "      <td>13.681521</td>\n",
       "      <td>13.686925</td>\n",
       "      <td>13.351959</td>\n",
       "      <td>13.185653</td>\n",
       "      <td>13.718821</td>\n",
       "      <td>...</td>\n",
       "      <td>12.822205</td>\n",
       "      <td>18.150157</td>\n",
       "      <td>13.196600</td>\n",
       "      <td>12.743566</td>\n",
       "      <td>13.133077</td>\n",
       "      <td>13.299443</td>\n",
       "      <td>14.267770</td>\n",
       "      <td>13.101010</td>\n",
       "      <td>14.140303</td>\n",
       "      <td>13.500263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>13.516266</td>\n",
       "      <td>13.282894</td>\n",
       "      <td>13.801476</td>\n",
       "      <td>13.370493</td>\n",
       "      <td>13.270262</td>\n",
       "      <td>14.120454</td>\n",
       "      <td>12.837948</td>\n",
       "      <td>20.118666</td>\n",
       "      <td>13.273351</td>\n",
       "      <td>13.580144</td>\n",
       "      <td>...</td>\n",
       "      <td>13.141555</td>\n",
       "      <td>13.346359</td>\n",
       "      <td>14.258582</td>\n",
       "      <td>13.126380</td>\n",
       "      <td>14.382816</td>\n",
       "      <td>13.355840</td>\n",
       "      <td>13.764570</td>\n",
       "      <td>14.449235</td>\n",
       "      <td>14.072590</td>\n",
       "      <td>14.245372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>14.546011</td>\n",
       "      <td>13.553623</td>\n",
       "      <td>19.783948</td>\n",
       "      <td>13.226588</td>\n",
       "      <td>13.415306</td>\n",
       "      <td>13.234058</td>\n",
       "      <td>15.354251</td>\n",
       "      <td>13.368636</td>\n",
       "      <td>12.863292</td>\n",
       "      <td>12.868586</td>\n",
       "      <td>...</td>\n",
       "      <td>13.632538</td>\n",
       "      <td>14.038220</td>\n",
       "      <td>13.149566</td>\n",
       "      <td>13.933306</td>\n",
       "      <td>12.921461</td>\n",
       "      <td>13.379677</td>\n",
       "      <td>12.801079</td>\n",
       "      <td>13.736506</td>\n",
       "      <td>21.453099</td>\n",
       "      <td>13.199691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13.146865</td>\n",
       "      <td>14.354633</td>\n",
       "      <td>13.270928</td>\n",
       "      <td>13.428343</td>\n",
       "      <td>13.406999</td>\n",
       "      <td>13.228120</td>\n",
       "      <td>13.241649</td>\n",
       "      <td>12.890681</td>\n",
       "      <td>12.897119</td>\n",
       "      <td>13.891413</td>\n",
       "      <td>...</td>\n",
       "      <td>13.806757</td>\n",
       "      <td>13.349812</td>\n",
       "      <td>12.833995</td>\n",
       "      <td>14.983345</td>\n",
       "      <td>14.401956</td>\n",
       "      <td>15.295731</td>\n",
       "      <td>13.424906</td>\n",
       "      <td>12.585087</td>\n",
       "      <td>14.583923</td>\n",
       "      <td>18.161130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13.535523</td>\n",
       "      <td>13.441005</td>\n",
       "      <td>13.085303</td>\n",
       "      <td>12.950349</td>\n",
       "      <td>13.519559</td>\n",
       "      <td>14.857715</td>\n",
       "      <td>13.137244</td>\n",
       "      <td>14.266478</td>\n",
       "      <td>15.235097</td>\n",
       "      <td>14.300004</td>\n",
       "      <td>...</td>\n",
       "      <td>14.128485</td>\n",
       "      <td>13.806800</td>\n",
       "      <td>13.403926</td>\n",
       "      <td>12.775390</td>\n",
       "      <td>13.360195</td>\n",
       "      <td>47.890248</td>\n",
       "      <td>13.928743</td>\n",
       "      <td>13.171176</td>\n",
       "      <td>13.697068</td>\n",
       "      <td>14.455355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14.275840</td>\n",
       "      <td>14.512551</td>\n",
       "      <td>13.859767</td>\n",
       "      <td>15.179249</td>\n",
       "      <td>12.908850</td>\n",
       "      <td>12.514672</td>\n",
       "      <td>13.156163</td>\n",
       "      <td>13.127043</td>\n",
       "      <td>13.007079</td>\n",
       "      <td>13.702954</td>\n",
       "      <td>...</td>\n",
       "      <td>13.344445</td>\n",
       "      <td>12.961218</td>\n",
       "      <td>12.977269</td>\n",
       "      <td>13.110183</td>\n",
       "      <td>13.102654</td>\n",
       "      <td>12.768557</td>\n",
       "      <td>13.451043</td>\n",
       "      <td>12.931767</td>\n",
       "      <td>16.617072</td>\n",
       "      <td>13.603479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>13.616763</td>\n",
       "      <td>15.848067</td>\n",
       "      <td>12.914329</td>\n",
       "      <td>13.063718</td>\n",
       "      <td>14.787972</td>\n",
       "      <td>14.698672</td>\n",
       "      <td>13.316950</td>\n",
       "      <td>13.434142</td>\n",
       "      <td>13.172001</td>\n",
       "      <td>17.033635</td>\n",
       "      <td>...</td>\n",
       "      <td>13.831477</td>\n",
       "      <td>13.463511</td>\n",
       "      <td>14.506132</td>\n",
       "      <td>12.844513</td>\n",
       "      <td>12.665541</td>\n",
       "      <td>13.809241</td>\n",
       "      <td>13.555800</td>\n",
       "      <td>14.552326</td>\n",
       "      <td>13.090457</td>\n",
       "      <td>13.091058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>14.583674</td>\n",
       "      <td>14.209304</td>\n",
       "      <td>19.921917</td>\n",
       "      <td>13.192207</td>\n",
       "      <td>17.256897</td>\n",
       "      <td>13.106388</td>\n",
       "      <td>13.265782</td>\n",
       "      <td>13.303717</td>\n",
       "      <td>13.116381</td>\n",
       "      <td>12.950952</td>\n",
       "      <td>...</td>\n",
       "      <td>14.835252</td>\n",
       "      <td>12.918394</td>\n",
       "      <td>14.202201</td>\n",
       "      <td>12.939890</td>\n",
       "      <td>13.587491</td>\n",
       "      <td>13.231722</td>\n",
       "      <td>13.591454</td>\n",
       "      <td>13.481726</td>\n",
       "      <td>12.772402</td>\n",
       "      <td>14.550613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>23.484754</td>\n",
       "      <td>13.495903</td>\n",
       "      <td>13.175757</td>\n",
       "      <td>15.713544</td>\n",
       "      <td>13.553592</td>\n",
       "      <td>13.263992</td>\n",
       "      <td>12.951173</td>\n",
       "      <td>13.411808</td>\n",
       "      <td>26.899655</td>\n",
       "      <td>13.156600</td>\n",
       "      <td>...</td>\n",
       "      <td>13.149264</td>\n",
       "      <td>13.524649</td>\n",
       "      <td>13.733397</td>\n",
       "      <td>13.325115</td>\n",
       "      <td>15.465600</td>\n",
       "      <td>12.948593</td>\n",
       "      <td>12.426875</td>\n",
       "      <td>12.952948</td>\n",
       "      <td>27.440435</td>\n",
       "      <td>12.727586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>13.065404</td>\n",
       "      <td>14.363278</td>\n",
       "      <td>13.748458</td>\n",
       "      <td>12.820139</td>\n",
       "      <td>13.354655</td>\n",
       "      <td>13.304882</td>\n",
       "      <td>13.043855</td>\n",
       "      <td>15.661626</td>\n",
       "      <td>13.046621</td>\n",
       "      <td>14.280985</td>\n",
       "      <td>...</td>\n",
       "      <td>13.397491</td>\n",
       "      <td>12.954797</td>\n",
       "      <td>12.743945</td>\n",
       "      <td>14.459516</td>\n",
       "      <td>12.936169</td>\n",
       "      <td>12.920073</td>\n",
       "      <td>14.421065</td>\n",
       "      <td>18.922717</td>\n",
       "      <td>13.280259</td>\n",
       "      <td>13.406568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>14.561894</td>\n",
       "      <td>13.076646</td>\n",
       "      <td>12.980112</td>\n",
       "      <td>13.102851</td>\n",
       "      <td>13.092108</td>\n",
       "      <td>13.499612</td>\n",
       "      <td>13.172990</td>\n",
       "      <td>12.939815</td>\n",
       "      <td>13.215742</td>\n",
       "      <td>12.852893</td>\n",
       "      <td>...</td>\n",
       "      <td>14.136926</td>\n",
       "      <td>12.874210</td>\n",
       "      <td>13.194134</td>\n",
       "      <td>14.089066</td>\n",
       "      <td>20.528107</td>\n",
       "      <td>12.817946</td>\n",
       "      <td>13.240449</td>\n",
       "      <td>12.877503</td>\n",
       "      <td>12.992077</td>\n",
       "      <td>12.913235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>13.170957</td>\n",
       "      <td>13.431852</td>\n",
       "      <td>13.030820</td>\n",
       "      <td>13.282643</td>\n",
       "      <td>13.082494</td>\n",
       "      <td>12.704183</td>\n",
       "      <td>18.108610</td>\n",
       "      <td>12.887467</td>\n",
       "      <td>13.860803</td>\n",
       "      <td>13.758408</td>\n",
       "      <td>...</td>\n",
       "      <td>14.653669</td>\n",
       "      <td>13.638623</td>\n",
       "      <td>16.090210</td>\n",
       "      <td>13.181756</td>\n",
       "      <td>13.563639</td>\n",
       "      <td>14.496622</td>\n",
       "      <td>12.788749</td>\n",
       "      <td>13.201896</td>\n",
       "      <td>13.095133</td>\n",
       "      <td>15.160427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>12.784575</td>\n",
       "      <td>12.918649</td>\n",
       "      <td>13.229628</td>\n",
       "      <td>17.271384</td>\n",
       "      <td>12.839159</td>\n",
       "      <td>12.877578</td>\n",
       "      <td>15.909897</td>\n",
       "      <td>13.767042</td>\n",
       "      <td>13.410037</td>\n",
       "      <td>13.448984</td>\n",
       "      <td>...</td>\n",
       "      <td>13.881780</td>\n",
       "      <td>17.078665</td>\n",
       "      <td>12.879003</td>\n",
       "      <td>13.330855</td>\n",
       "      <td>12.291128</td>\n",
       "      <td>13.167822</td>\n",
       "      <td>13.466594</td>\n",
       "      <td>13.066105</td>\n",
       "      <td>12.580700</td>\n",
       "      <td>13.540842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>13.507791</td>\n",
       "      <td>13.172250</td>\n",
       "      <td>19.515425</td>\n",
       "      <td>14.311644</td>\n",
       "      <td>16.811739</td>\n",
       "      <td>13.942457</td>\n",
       "      <td>13.079623</td>\n",
       "      <td>13.952572</td>\n",
       "      <td>14.021757</td>\n",
       "      <td>13.042533</td>\n",
       "      <td>...</td>\n",
       "      <td>12.896813</td>\n",
       "      <td>12.901370</td>\n",
       "      <td>12.934241</td>\n",
       "      <td>13.522908</td>\n",
       "      <td>13.084717</td>\n",
       "      <td>13.773916</td>\n",
       "      <td>13.778416</td>\n",
       "      <td>13.322324</td>\n",
       "      <td>13.199064</td>\n",
       "      <td>13.243959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>13.592067</td>\n",
       "      <td>12.976225</td>\n",
       "      <td>22.949725</td>\n",
       "      <td>13.011905</td>\n",
       "      <td>14.332263</td>\n",
       "      <td>13.078481</td>\n",
       "      <td>15.385479</td>\n",
       "      <td>13.319365</td>\n",
       "      <td>14.143923</td>\n",
       "      <td>13.174485</td>\n",
       "      <td>...</td>\n",
       "      <td>12.984245</td>\n",
       "      <td>13.610552</td>\n",
       "      <td>36.976805</td>\n",
       "      <td>12.741621</td>\n",
       "      <td>12.633232</td>\n",
       "      <td>18.599167</td>\n",
       "      <td>13.991299</td>\n",
       "      <td>13.574328</td>\n",
       "      <td>14.106030</td>\n",
       "      <td>12.918337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>13.077372</td>\n",
       "      <td>14.586627</td>\n",
       "      <td>13.815221</td>\n",
       "      <td>13.212594</td>\n",
       "      <td>13.844573</td>\n",
       "      <td>13.333818</td>\n",
       "      <td>12.801714</td>\n",
       "      <td>13.983800</td>\n",
       "      <td>12.563403</td>\n",
       "      <td>13.560591</td>\n",
       "      <td>...</td>\n",
       "      <td>12.954342</td>\n",
       "      <td>13.422393</td>\n",
       "      <td>13.755528</td>\n",
       "      <td>14.291317</td>\n",
       "      <td>13.216468</td>\n",
       "      <td>13.092205</td>\n",
       "      <td>12.942221</td>\n",
       "      <td>15.511429</td>\n",
       "      <td>12.846752</td>\n",
       "      <td>12.579851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>13.734471</td>\n",
       "      <td>13.210579</td>\n",
       "      <td>14.648193</td>\n",
       "      <td>13.298417</td>\n",
       "      <td>13.504851</td>\n",
       "      <td>13.258675</td>\n",
       "      <td>13.530548</td>\n",
       "      <td>12.686022</td>\n",
       "      <td>12.818924</td>\n",
       "      <td>13.328165</td>\n",
       "      <td>...</td>\n",
       "      <td>12.956107</td>\n",
       "      <td>13.412025</td>\n",
       "      <td>12.660967</td>\n",
       "      <td>13.239494</td>\n",
       "      <td>13.478017</td>\n",
       "      <td>13.178266</td>\n",
       "      <td>12.978762</td>\n",
       "      <td>13.451341</td>\n",
       "      <td>27.228255</td>\n",
       "      <td>13.028459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>13.371576</td>\n",
       "      <td>13.794552</td>\n",
       "      <td>13.333609</td>\n",
       "      <td>12.765062</td>\n",
       "      <td>13.167494</td>\n",
       "      <td>13.082520</td>\n",
       "      <td>12.674479</td>\n",
       "      <td>13.413838</td>\n",
       "      <td>13.523360</td>\n",
       "      <td>13.182469</td>\n",
       "      <td>...</td>\n",
       "      <td>13.480756</td>\n",
       "      <td>12.721732</td>\n",
       "      <td>15.120597</td>\n",
       "      <td>12.949071</td>\n",
       "      <td>13.175276</td>\n",
       "      <td>12.802942</td>\n",
       "      <td>13.166319</td>\n",
       "      <td>13.116030</td>\n",
       "      <td>12.921249</td>\n",
       "      <td>13.003810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>19.672869</td>\n",
       "      <td>12.829295</td>\n",
       "      <td>13.615077</td>\n",
       "      <td>13.073961</td>\n",
       "      <td>17.107927</td>\n",
       "      <td>13.100274</td>\n",
       "      <td>13.057248</td>\n",
       "      <td>12.874028</td>\n",
       "      <td>13.131262</td>\n",
       "      <td>12.727048</td>\n",
       "      <td>...</td>\n",
       "      <td>13.293985</td>\n",
       "      <td>14.743216</td>\n",
       "      <td>15.316338</td>\n",
       "      <td>13.909703</td>\n",
       "      <td>20.183615</td>\n",
       "      <td>12.804092</td>\n",
       "      <td>13.023582</td>\n",
       "      <td>13.498230</td>\n",
       "      <td>19.010826</td>\n",
       "      <td>13.067448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>13.289264</td>\n",
       "      <td>13.392319</td>\n",
       "      <td>26.020998</td>\n",
       "      <td>13.896190</td>\n",
       "      <td>13.106595</td>\n",
       "      <td>16.827638</td>\n",
       "      <td>13.103907</td>\n",
       "      <td>13.275481</td>\n",
       "      <td>12.993699</td>\n",
       "      <td>13.377099</td>\n",
       "      <td>...</td>\n",
       "      <td>17.353748</td>\n",
       "      <td>13.408610</td>\n",
       "      <td>13.621906</td>\n",
       "      <td>12.822259</td>\n",
       "      <td>23.422046</td>\n",
       "      <td>12.938950</td>\n",
       "      <td>13.493311</td>\n",
       "      <td>13.691217</td>\n",
       "      <td>14.442297</td>\n",
       "      <td>13.482952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>15.787226</td>\n",
       "      <td>12.812689</td>\n",
       "      <td>13.542323</td>\n",
       "      <td>12.677718</td>\n",
       "      <td>13.274587</td>\n",
       "      <td>13.510142</td>\n",
       "      <td>13.342584</td>\n",
       "      <td>13.191206</td>\n",
       "      <td>12.832243</td>\n",
       "      <td>13.056207</td>\n",
       "      <td>...</td>\n",
       "      <td>13.060355</td>\n",
       "      <td>13.299164</td>\n",
       "      <td>58.316402</td>\n",
       "      <td>42.778163</td>\n",
       "      <td>13.125853</td>\n",
       "      <td>13.197110</td>\n",
       "      <td>15.253229</td>\n",
       "      <td>12.527827</td>\n",
       "      <td>13.751084</td>\n",
       "      <td>15.150946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>20.666208</td>\n",
       "      <td>12.716799</td>\n",
       "      <td>14.601839</td>\n",
       "      <td>12.904401</td>\n",
       "      <td>13.077012</td>\n",
       "      <td>12.518163</td>\n",
       "      <td>14.325405</td>\n",
       "      <td>15.911566</td>\n",
       "      <td>13.856244</td>\n",
       "      <td>12.854663</td>\n",
       "      <td>...</td>\n",
       "      <td>12.927004</td>\n",
       "      <td>12.722435</td>\n",
       "      <td>13.364313</td>\n",
       "      <td>12.618326</td>\n",
       "      <td>14.262271</td>\n",
       "      <td>12.979275</td>\n",
       "      <td>12.543611</td>\n",
       "      <td>13.176223</td>\n",
       "      <td>13.745837</td>\n",
       "      <td>13.418222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>12.605889</td>\n",
       "      <td>13.193941</td>\n",
       "      <td>14.511955</td>\n",
       "      <td>13.240881</td>\n",
       "      <td>12.928420</td>\n",
       "      <td>14.423788</td>\n",
       "      <td>13.140722</td>\n",
       "      <td>12.758665</td>\n",
       "      <td>13.506108</td>\n",
       "      <td>12.961010</td>\n",
       "      <td>...</td>\n",
       "      <td>13.324977</td>\n",
       "      <td>13.340030</td>\n",
       "      <td>12.533177</td>\n",
       "      <td>13.547514</td>\n",
       "      <td>13.649756</td>\n",
       "      <td>13.104918</td>\n",
       "      <td>13.600401</td>\n",
       "      <td>33.008297</td>\n",
       "      <td>15.854512</td>\n",
       "      <td>13.280333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     column_0   column_1   column_2   column_3   column_4   column_5  \\\n",
       "0   13.447132  13.037364  13.320732  13.934092  14.160968  12.997231   \n",
       "1   12.799192  15.720345  12.893337  13.209815  13.018760  12.853811   \n",
       "2   14.011855  14.344862  13.188338  14.408124  13.020654  13.505918   \n",
       "3   13.223301  13.600391  12.934800  17.042988  15.969407  46.141069   \n",
       "4   12.462461  32.868870  13.565033  12.948105  13.117695  13.190062   \n",
       "5   13.029042  20.261400  13.222501  13.414526  13.265013  13.911818   \n",
       "6   13.288539  17.563226  13.203073  14.250993  16.465606  13.475710   \n",
       "7   13.698956  12.809229  13.616637  13.576106  13.425336  12.761027   \n",
       "8   14.219683  13.933556  13.126639  13.369487  13.163664  12.818119   \n",
       "9   17.730966  14.513986  13.172437  13.306472  13.168511  13.681521   \n",
       "10  13.516266  13.282894  13.801476  13.370493  13.270262  14.120454   \n",
       "11  14.546011  13.553623  19.783948  13.226588  13.415306  13.234058   \n",
       "12  13.146865  14.354633  13.270928  13.428343  13.406999  13.228120   \n",
       "13  13.535523  13.441005  13.085303  12.950349  13.519559  14.857715   \n",
       "14  14.275840  14.512551  13.859767  15.179249  12.908850  12.514672   \n",
       "15  13.616763  15.848067  12.914329  13.063718  14.787972  14.698672   \n",
       "16  14.583674  14.209304  19.921917  13.192207  17.256897  13.106388   \n",
       "17  23.484754  13.495903  13.175757  15.713544  13.553592  13.263992   \n",
       "18  13.065404  14.363278  13.748458  12.820139  13.354655  13.304882   \n",
       "19  14.561894  13.076646  12.980112  13.102851  13.092108  13.499612   \n",
       "20  13.170957  13.431852  13.030820  13.282643  13.082494  12.704183   \n",
       "21  12.784575  12.918649  13.229628  17.271384  12.839159  12.877578   \n",
       "22  13.507791  13.172250  19.515425  14.311644  16.811739  13.942457   \n",
       "23  13.592067  12.976225  22.949725  13.011905  14.332263  13.078481   \n",
       "24  13.077372  14.586627  13.815221  13.212594  13.844573  13.333818   \n",
       "25  13.734471  13.210579  14.648193  13.298417  13.504851  13.258675   \n",
       "26  13.371576  13.794552  13.333609  12.765062  13.167494  13.082520   \n",
       "27  19.672869  12.829295  13.615077  13.073961  17.107927  13.100274   \n",
       "28  13.289264  13.392319  26.020998  13.896190  13.106595  16.827638   \n",
       "29  15.787226  12.812689  13.542323  12.677718  13.274587  13.510142   \n",
       "30  20.666208  12.716799  14.601839  12.904401  13.077012  12.518163   \n",
       "31  12.605889  13.193941  14.511955  13.240881  12.928420  14.423788   \n",
       "\n",
       "     column_6   column_7   column_8   column_9  ...  column_22  column_23  \\\n",
       "0   13.605174  13.179105  13.031221  13.514571  ...  12.830434  13.167031   \n",
       "1   14.092373  28.917124  14.327434  12.952086  ...  13.051660  13.315811   \n",
       "2   12.732587  12.829406  12.820350  13.804098  ...  13.279897  13.659994   \n",
       "3   13.909606  13.255826  14.416834  12.676454  ...  12.976551  12.928398   \n",
       "4   13.625904  12.907982  14.176126  14.645302  ...  13.271953  13.048486   \n",
       "5   13.364908  13.694575  12.849393  12.926298  ...  13.101895  14.444280   \n",
       "6   13.174175  13.166318  13.590350  13.119641  ...  12.936764  17.106738   \n",
       "7   14.480694  13.610478  13.357413  13.475321  ...  14.503129  12.979057   \n",
       "8   13.740463  12.767789  32.232798  12.942297  ...  13.261890  12.887265   \n",
       "9   13.686925  13.351959  13.185653  13.718821  ...  12.822205  18.150157   \n",
       "10  12.837948  20.118666  13.273351  13.580144  ...  13.141555  13.346359   \n",
       "11  15.354251  13.368636  12.863292  12.868586  ...  13.632538  14.038220   \n",
       "12  13.241649  12.890681  12.897119  13.891413  ...  13.806757  13.349812   \n",
       "13  13.137244  14.266478  15.235097  14.300004  ...  14.128485  13.806800   \n",
       "14  13.156163  13.127043  13.007079  13.702954  ...  13.344445  12.961218   \n",
       "15  13.316950  13.434142  13.172001  17.033635  ...  13.831477  13.463511   \n",
       "16  13.265782  13.303717  13.116381  12.950952  ...  14.835252  12.918394   \n",
       "17  12.951173  13.411808  26.899655  13.156600  ...  13.149264  13.524649   \n",
       "18  13.043855  15.661626  13.046621  14.280985  ...  13.397491  12.954797   \n",
       "19  13.172990  12.939815  13.215742  12.852893  ...  14.136926  12.874210   \n",
       "20  18.108610  12.887467  13.860803  13.758408  ...  14.653669  13.638623   \n",
       "21  15.909897  13.767042  13.410037  13.448984  ...  13.881780  17.078665   \n",
       "22  13.079623  13.952572  14.021757  13.042533  ...  12.896813  12.901370   \n",
       "23  15.385479  13.319365  14.143923  13.174485  ...  12.984245  13.610552   \n",
       "24  12.801714  13.983800  12.563403  13.560591  ...  12.954342  13.422393   \n",
       "25  13.530548  12.686022  12.818924  13.328165  ...  12.956107  13.412025   \n",
       "26  12.674479  13.413838  13.523360  13.182469  ...  13.480756  12.721732   \n",
       "27  13.057248  12.874028  13.131262  12.727048  ...  13.293985  14.743216   \n",
       "28  13.103907  13.275481  12.993699  13.377099  ...  17.353748  13.408610   \n",
       "29  13.342584  13.191206  12.832243  13.056207  ...  13.060355  13.299164   \n",
       "30  14.325405  15.911566  13.856244  12.854663  ...  12.927004  12.722435   \n",
       "31  13.140722  12.758665  13.506108  12.961010  ...  13.324977  13.340030   \n",
       "\n",
       "    column_24  column_25  column_26  column_27  column_28  column_29  \\\n",
       "0   12.889489  13.446304  13.519172  13.556801  13.398178  12.817820   \n",
       "1   12.645670  13.236526  12.655067  13.625855  15.532673  13.316868   \n",
       "2   12.731043  14.440821  13.113338  12.870840  13.968151  15.748286   \n",
       "3   13.247288  13.112748  13.768122  13.077388  13.206464  12.941768   \n",
       "4   14.321457  17.067648  13.212197  13.434782  13.049322  13.067504   \n",
       "5   12.707569  15.358972  18.928262  13.020566  13.191545  13.343338   \n",
       "6   13.071521  13.183591  13.152070  15.829402  12.605497  16.573056   \n",
       "7   13.687640  13.118037  33.735586  13.335105  13.800307  13.748396   \n",
       "8   13.089404  12.941111  13.672106  13.989907  13.435668  14.519015   \n",
       "9   13.196600  12.743566  13.133077  13.299443  14.267770  13.101010   \n",
       "10  14.258582  13.126380  14.382816  13.355840  13.764570  14.449235   \n",
       "11  13.149566  13.933306  12.921461  13.379677  12.801079  13.736506   \n",
       "12  12.833995  14.983345  14.401956  15.295731  13.424906  12.585087   \n",
       "13  13.403926  12.775390  13.360195  47.890248  13.928743  13.171176   \n",
       "14  12.977269  13.110183  13.102654  12.768557  13.451043  12.931767   \n",
       "15  14.506132  12.844513  12.665541  13.809241  13.555800  14.552326   \n",
       "16  14.202201  12.939890  13.587491  13.231722  13.591454  13.481726   \n",
       "17  13.733397  13.325115  15.465600  12.948593  12.426875  12.952948   \n",
       "18  12.743945  14.459516  12.936169  12.920073  14.421065  18.922717   \n",
       "19  13.194134  14.089066  20.528107  12.817946  13.240449  12.877503   \n",
       "20  16.090210  13.181756  13.563639  14.496622  12.788749  13.201896   \n",
       "21  12.879003  13.330855  12.291128  13.167822  13.466594  13.066105   \n",
       "22  12.934241  13.522908  13.084717  13.773916  13.778416  13.322324   \n",
       "23  36.976805  12.741621  12.633232  18.599167  13.991299  13.574328   \n",
       "24  13.755528  14.291317  13.216468  13.092205  12.942221  15.511429   \n",
       "25  12.660967  13.239494  13.478017  13.178266  12.978762  13.451341   \n",
       "26  15.120597  12.949071  13.175276  12.802942  13.166319  13.116030   \n",
       "27  15.316338  13.909703  20.183615  12.804092  13.023582  13.498230   \n",
       "28  13.621906  12.822259  23.422046  12.938950  13.493311  13.691217   \n",
       "29  58.316402  42.778163  13.125853  13.197110  15.253229  12.527827   \n",
       "30  13.364313  12.618326  14.262271  12.979275  12.543611  13.176223   \n",
       "31  12.533177  13.547514  13.649756  13.104918  13.600401  33.008297   \n",
       "\n",
       "    column_30  column_31  \n",
       "0   12.855377  13.383958  \n",
       "1   13.065052  13.007496  \n",
       "2   13.902305  12.735503  \n",
       "3   13.240524  12.603768  \n",
       "4   12.892875  12.835842  \n",
       "5   13.207074  12.902596  \n",
       "6   13.396213  13.353961  \n",
       "7   15.201048  13.346287  \n",
       "8   13.414396  14.830492  \n",
       "9   14.140303  13.500263  \n",
       "10  14.072590  14.245372  \n",
       "11  21.453099  13.199691  \n",
       "12  14.583923  18.161130  \n",
       "13  13.697068  14.455355  \n",
       "14  16.617072  13.603479  \n",
       "15  13.090457  13.091058  \n",
       "16  12.772402  14.550613  \n",
       "17  27.440435  12.727586  \n",
       "18  13.280259  13.406568  \n",
       "19  12.992077  12.913235  \n",
       "20  13.095133  15.160427  \n",
       "21  12.580700  13.540842  \n",
       "22  13.199064  13.243959  \n",
       "23  14.106030  12.918337  \n",
       "24  12.846752  12.579851  \n",
       "25  27.228255  13.028459  \n",
       "26  12.921249  13.003810  \n",
       "27  19.010826  13.067448  \n",
       "28  14.442297  13.482952  \n",
       "29  13.751084  15.150946  \n",
       "30  13.745837  13.418222  \n",
       "31  15.854512  13.280333  \n",
       "\n",
       "[32 rows x 32 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_parquet(r'C:\\Users\\siddh\\Desktop\\deep\\Ariel-Data-Challenge-2024\\Data\\ariel-data-challenge-2024\\train\\785834\\FGS1_calibration\\read.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67500,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cum_signal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\siddh\\Desktop\\deep\\Ariel-Data-Challenge-2024\\Data\\ariel-data-challenge-2024\\train_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>planet_id</th>\n",
       "      <th>wl_1</th>\n",
       "      <th>wl_2</th>\n",
       "      <th>wl_3</th>\n",
       "      <th>wl_4</th>\n",
       "      <th>wl_5</th>\n",
       "      <th>wl_6</th>\n",
       "      <th>wl_7</th>\n",
       "      <th>wl_8</th>\n",
       "      <th>wl_9</th>\n",
       "      <th>...</th>\n",
       "      <th>wl_274</th>\n",
       "      <th>wl_275</th>\n",
       "      <th>wl_276</th>\n",
       "      <th>wl_277</th>\n",
       "      <th>wl_278</th>\n",
       "      <th>wl_279</th>\n",
       "      <th>wl_280</th>\n",
       "      <th>wl_281</th>\n",
       "      <th>wl_282</th>\n",
       "      <th>wl_283</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>785834</td>\n",
       "      <td>0.001086</td>\n",
       "      <td>0.001137</td>\n",
       "      <td>0.001131</td>\n",
       "      <td>0.001124</td>\n",
       "      <td>0.001138</td>\n",
       "      <td>0.001131</td>\n",
       "      <td>0.001123</td>\n",
       "      <td>0.001127</td>\n",
       "      <td>0.001120</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001075</td>\n",
       "      <td>0.001076</td>\n",
       "      <td>0.001076</td>\n",
       "      <td>0.001076</td>\n",
       "      <td>0.001074</td>\n",
       "      <td>0.001073</td>\n",
       "      <td>0.001072</td>\n",
       "      <td>0.001073</td>\n",
       "      <td>0.001073</td>\n",
       "      <td>0.001072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14485303</td>\n",
       "      <td>0.001835</td>\n",
       "      <td>0.001835</td>\n",
       "      <td>0.001834</td>\n",
       "      <td>0.001833</td>\n",
       "      <td>0.001833</td>\n",
       "      <td>0.001833</td>\n",
       "      <td>0.001833</td>\n",
       "      <td>0.001834</td>\n",
       "      <td>0.001834</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001893</td>\n",
       "      <td>0.001892</td>\n",
       "      <td>0.001892</td>\n",
       "      <td>0.001891</td>\n",
       "      <td>0.001891</td>\n",
       "      <td>0.001891</td>\n",
       "      <td>0.001890</td>\n",
       "      <td>0.001890</td>\n",
       "      <td>0.001889</td>\n",
       "      <td>0.001888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17002355</td>\n",
       "      <td>0.002792</td>\n",
       "      <td>0.002814</td>\n",
       "      <td>0.002808</td>\n",
       "      <td>0.002804</td>\n",
       "      <td>0.002809</td>\n",
       "      <td>0.002805</td>\n",
       "      <td>0.002802</td>\n",
       "      <td>0.002805</td>\n",
       "      <td>0.002801</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002784</td>\n",
       "      <td>0.002783</td>\n",
       "      <td>0.002783</td>\n",
       "      <td>0.002783</td>\n",
       "      <td>0.002783</td>\n",
       "      <td>0.002784</td>\n",
       "      <td>0.002784</td>\n",
       "      <td>0.002785</td>\n",
       "      <td>0.002785</td>\n",
       "      <td>0.002784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24135240</td>\n",
       "      <td>0.001294</td>\n",
       "      <td>0.001308</td>\n",
       "      <td>0.001308</td>\n",
       "      <td>0.001306</td>\n",
       "      <td>0.001306</td>\n",
       "      <td>0.001303</td>\n",
       "      <td>0.001306</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001405</td>\n",
       "      <td>0.001404</td>\n",
       "      <td>0.001403</td>\n",
       "      <td>0.001402</td>\n",
       "      <td>0.001401</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>0.001397</td>\n",
       "      <td>0.001395</td>\n",
       "      <td>0.001393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25070640</td>\n",
       "      <td>0.001987</td>\n",
       "      <td>0.001987</td>\n",
       "      <td>0.001987</td>\n",
       "      <td>0.001987</td>\n",
       "      <td>0.001987</td>\n",
       "      <td>0.001987</td>\n",
       "      <td>0.001987</td>\n",
       "      <td>0.001987</td>\n",
       "      <td>0.001987</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001988</td>\n",
       "      <td>0.001988</td>\n",
       "      <td>0.001988</td>\n",
       "      <td>0.001988</td>\n",
       "      <td>0.001988</td>\n",
       "      <td>0.001988</td>\n",
       "      <td>0.001988</td>\n",
       "      <td>0.001988</td>\n",
       "      <td>0.001988</td>\n",
       "      <td>0.001988</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 284 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   planet_id      wl_1      wl_2      wl_3      wl_4      wl_5      wl_6  \\\n",
       "0     785834  0.001086  0.001137  0.001131  0.001124  0.001138  0.001131   \n",
       "1   14485303  0.001835  0.001835  0.001834  0.001833  0.001833  0.001833   \n",
       "2   17002355  0.002792  0.002814  0.002808  0.002804  0.002809  0.002805   \n",
       "3   24135240  0.001294  0.001308  0.001308  0.001306  0.001306  0.001303   \n",
       "4   25070640  0.001987  0.001987  0.001987  0.001987  0.001987  0.001987   \n",
       "\n",
       "       wl_7      wl_8      wl_9  ...    wl_274    wl_275    wl_276    wl_277  \\\n",
       "0  0.001123  0.001127  0.001120  ...  0.001075  0.001076  0.001076  0.001076   \n",
       "1  0.001833  0.001834  0.001834  ...  0.001893  0.001892  0.001892  0.001891   \n",
       "2  0.002802  0.002805  0.002801  ...  0.002784  0.002783  0.002783  0.002783   \n",
       "3  0.001306  0.001314  0.001314  ...  0.001405  0.001404  0.001403  0.001402   \n",
       "4  0.001987  0.001987  0.001987  ...  0.001988  0.001988  0.001988  0.001988   \n",
       "\n",
       "     wl_278    wl_279    wl_280    wl_281    wl_282    wl_283  \n",
       "0  0.001074  0.001073  0.001072  0.001073  0.001073  0.001072  \n",
       "1  0.001891  0.001891  0.001890  0.001890  0.001889  0.001888  \n",
       "2  0.002783  0.002784  0.002784  0.002785  0.002785  0.002784  \n",
       "3  0.001401  0.001400  0.001399  0.001397  0.001395  0.001393  \n",
       "4  0.001988  0.001988  0.001988  0.001988  0.001988  0.001988  \n",
       "\n",
       "[5 rows x 284 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wl_1 = df['wl_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([143., 164., 112.,  57.,  40.,  38.,  31.,  45.,  29.,  14.]),\n",
       " array([0.00039627, 0.0010663 , 0.00173633, 0.00240636, 0.00307639,\n",
       "        0.00374642, 0.00441645, 0.00508647, 0.0057565 , 0.00642653,\n",
       "        0.00709656]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(wl_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>planet_id</th>\n",
       "      <th>wl_1</th>\n",
       "      <th>wl_2</th>\n",
       "      <th>wl_3</th>\n",
       "      <th>wl_4</th>\n",
       "      <th>wl_5</th>\n",
       "      <th>wl_6</th>\n",
       "      <th>wl_7</th>\n",
       "      <th>wl_8</th>\n",
       "      <th>wl_9</th>\n",
       "      <th>...</th>\n",
       "      <th>wl_274</th>\n",
       "      <th>wl_275</th>\n",
       "      <th>wl_276</th>\n",
       "      <th>wl_277</th>\n",
       "      <th>wl_278</th>\n",
       "      <th>wl_279</th>\n",
       "      <th>wl_280</th>\n",
       "      <th>wl_281</th>\n",
       "      <th>wl_282</th>\n",
       "      <th>wl_283</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6.730000e+02</td>\n",
       "      <td>673.000000</td>\n",
       "      <td>673.000000</td>\n",
       "      <td>673.000000</td>\n",
       "      <td>673.000000</td>\n",
       "      <td>673.000000</td>\n",
       "      <td>673.000000</td>\n",
       "      <td>673.000000</td>\n",
       "      <td>673.000000</td>\n",
       "      <td>673.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>673.000000</td>\n",
       "      <td>673.000000</td>\n",
       "      <td>673.000000</td>\n",
       "      <td>673.000000</td>\n",
       "      <td>673.000000</td>\n",
       "      <td>673.000000</td>\n",
       "      <td>673.000000</td>\n",
       "      <td>673.000000</td>\n",
       "      <td>673.000000</td>\n",
       "      <td>673.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.131767e+09</td>\n",
       "      <td>0.002486</td>\n",
       "      <td>0.002516</td>\n",
       "      <td>0.002510</td>\n",
       "      <td>0.002505</td>\n",
       "      <td>0.002507</td>\n",
       "      <td>0.002499</td>\n",
       "      <td>0.002499</td>\n",
       "      <td>0.002508</td>\n",
       "      <td>0.002505</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002513</td>\n",
       "      <td>0.002513</td>\n",
       "      <td>0.002512</td>\n",
       "      <td>0.002512</td>\n",
       "      <td>0.002512</td>\n",
       "      <td>0.002513</td>\n",
       "      <td>0.002513</td>\n",
       "      <td>0.002512</td>\n",
       "      <td>0.002511</td>\n",
       "      <td>0.002510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.200253e+09</td>\n",
       "      <td>0.001720</td>\n",
       "      <td>0.001720</td>\n",
       "      <td>0.001717</td>\n",
       "      <td>0.001717</td>\n",
       "      <td>0.001715</td>\n",
       "      <td>0.001713</td>\n",
       "      <td>0.001714</td>\n",
       "      <td>0.001716</td>\n",
       "      <td>0.001716</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001721</td>\n",
       "      <td>0.001721</td>\n",
       "      <td>0.001721</td>\n",
       "      <td>0.001721</td>\n",
       "      <td>0.001721</td>\n",
       "      <td>0.001722</td>\n",
       "      <td>0.001722</td>\n",
       "      <td>0.001722</td>\n",
       "      <td>0.001722</td>\n",
       "      <td>0.001721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>7.858340e+05</td>\n",
       "      <td>0.000396</td>\n",
       "      <td>0.000396</td>\n",
       "      <td>0.000396</td>\n",
       "      <td>0.000396</td>\n",
       "      <td>0.000396</td>\n",
       "      <td>0.000396</td>\n",
       "      <td>0.000396</td>\n",
       "      <td>0.000396</td>\n",
       "      <td>0.000396</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>0.000397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.121250e+09</td>\n",
       "      <td>0.001183</td>\n",
       "      <td>0.001206</td>\n",
       "      <td>0.001206</td>\n",
       "      <td>0.001206</td>\n",
       "      <td>0.001206</td>\n",
       "      <td>0.001206</td>\n",
       "      <td>0.001206</td>\n",
       "      <td>0.001206</td>\n",
       "      <td>0.001206</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001207</td>\n",
       "      <td>0.001207</td>\n",
       "      <td>0.001206</td>\n",
       "      <td>0.001206</td>\n",
       "      <td>0.001206</td>\n",
       "      <td>0.001206</td>\n",
       "      <td>0.001206</td>\n",
       "      <td>0.001206</td>\n",
       "      <td>0.001206</td>\n",
       "      <td>0.001206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.042817e+09</td>\n",
       "      <td>0.001866</td>\n",
       "      <td>0.001905</td>\n",
       "      <td>0.001896</td>\n",
       "      <td>0.001894</td>\n",
       "      <td>0.001896</td>\n",
       "      <td>0.001894</td>\n",
       "      <td>0.001886</td>\n",
       "      <td>0.001896</td>\n",
       "      <td>0.001895</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001897</td>\n",
       "      <td>0.001897</td>\n",
       "      <td>0.001896</td>\n",
       "      <td>0.001896</td>\n",
       "      <td>0.001896</td>\n",
       "      <td>0.001897</td>\n",
       "      <td>0.001897</td>\n",
       "      <td>0.001897</td>\n",
       "      <td>0.001897</td>\n",
       "      <td>0.001896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.101987e+09</td>\n",
       "      <td>0.003527</td>\n",
       "      <td>0.003533</td>\n",
       "      <td>0.003530</td>\n",
       "      <td>0.003529</td>\n",
       "      <td>0.003528</td>\n",
       "      <td>0.003527</td>\n",
       "      <td>0.003529</td>\n",
       "      <td>0.003532</td>\n",
       "      <td>0.003532</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003529</td>\n",
       "      <td>0.003529</td>\n",
       "      <td>0.003529</td>\n",
       "      <td>0.003529</td>\n",
       "      <td>0.003529</td>\n",
       "      <td>0.003529</td>\n",
       "      <td>0.003529</td>\n",
       "      <td>0.003529</td>\n",
       "      <td>0.003529</td>\n",
       "      <td>0.003528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.286134e+09</td>\n",
       "      <td>0.007097</td>\n",
       "      <td>0.007080</td>\n",
       "      <td>0.007077</td>\n",
       "      <td>0.007072</td>\n",
       "      <td>0.007071</td>\n",
       "      <td>0.007069</td>\n",
       "      <td>0.007070</td>\n",
       "      <td>0.007072</td>\n",
       "      <td>0.007071</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007233</td>\n",
       "      <td>0.007227</td>\n",
       "      <td>0.007222</td>\n",
       "      <td>0.007223</td>\n",
       "      <td>0.007231</td>\n",
       "      <td>0.007238</td>\n",
       "      <td>0.007239</td>\n",
       "      <td>0.007233</td>\n",
       "      <td>0.007224</td>\n",
       "      <td>0.007217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 284 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          planet_id        wl_1        wl_2        wl_3        wl_4  \\\n",
       "count  6.730000e+02  673.000000  673.000000  673.000000  673.000000   \n",
       "mean   2.131767e+09    0.002486    0.002516    0.002510    0.002505   \n",
       "std    1.200253e+09    0.001720    0.001720    0.001717    0.001717   \n",
       "min    7.858340e+05    0.000396    0.000396    0.000396    0.000396   \n",
       "25%    1.121250e+09    0.001183    0.001206    0.001206    0.001206   \n",
       "50%    2.042817e+09    0.001866    0.001905    0.001896    0.001894   \n",
       "75%    3.101987e+09    0.003527    0.003533    0.003530    0.003529   \n",
       "max    4.286134e+09    0.007097    0.007080    0.007077    0.007072   \n",
       "\n",
       "             wl_5        wl_6        wl_7        wl_8        wl_9  ...  \\\n",
       "count  673.000000  673.000000  673.000000  673.000000  673.000000  ...   \n",
       "mean     0.002507    0.002499    0.002499    0.002508    0.002505  ...   \n",
       "std      0.001715    0.001713    0.001714    0.001716    0.001716  ...   \n",
       "min      0.000396    0.000396    0.000396    0.000396    0.000396  ...   \n",
       "25%      0.001206    0.001206    0.001206    0.001206    0.001206  ...   \n",
       "50%      0.001896    0.001894    0.001886    0.001896    0.001895  ...   \n",
       "75%      0.003528    0.003527    0.003529    0.003532    0.003532  ...   \n",
       "max      0.007071    0.007069    0.007070    0.007072    0.007071  ...   \n",
       "\n",
       "           wl_274      wl_275      wl_276      wl_277      wl_278      wl_279  \\\n",
       "count  673.000000  673.000000  673.000000  673.000000  673.000000  673.000000   \n",
       "mean     0.002513    0.002513    0.002512    0.002512    0.002512    0.002513   \n",
       "std      0.001721    0.001721    0.001721    0.001721    0.001721    0.001722   \n",
       "min      0.000397    0.000397    0.000397    0.000397    0.000397    0.000397   \n",
       "25%      0.001207    0.001207    0.001206    0.001206    0.001206    0.001206   \n",
       "50%      0.001897    0.001897    0.001896    0.001896    0.001896    0.001897   \n",
       "75%      0.003529    0.003529    0.003529    0.003529    0.003529    0.003529   \n",
       "max      0.007233    0.007227    0.007222    0.007223    0.007231    0.007238   \n",
       "\n",
       "           wl_280      wl_281      wl_282      wl_283  \n",
       "count  673.000000  673.000000  673.000000  673.000000  \n",
       "mean     0.002513    0.002512    0.002511    0.002510  \n",
       "std      0.001722    0.001722    0.001722    0.001721  \n",
       "min      0.000397    0.000397    0.000397    0.000397  \n",
       "25%      0.001206    0.001206    0.001206    0.001206  \n",
       "50%      0.001897    0.001897    0.001897    0.001896  \n",
       "75%      0.003529    0.003529    0.003529    0.003528  \n",
       "max      0.007239    0.007233    0.007224    0.007217  \n",
       "\n",
       "[8 rows x 284 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
