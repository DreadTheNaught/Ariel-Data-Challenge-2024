  0%|                                                                  | 0/17 [00:00<?, ?it/s]C:\Users\prana\Documents\Python_Stuff\Ariel-Data-Challenge-2024\models\BasicCNN.py:77: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\native\TensorShape.cpp:3701.)
torch.Size([32, 187, 32, 32]) torch.Size([32, 187, 282, 32])
  x = x.T
  0%|                                                                  | 0/17 [00:31<?, ?it/s]
Traceback (most recent call last):
  File "C:\Users\prana\Documents\Python_Stuff\Ariel-Data-Challenge-2024\main.py", line 65, in <module>
    train_valid_test(model = CNN(), train_loader = train_loader, valid_loader = valid_loader, loss_func = loss_func)
  File "C:\Users\prana\Documents\Python_Stuff\Ariel-Data-Challenge-2024\trainer.py", line 64, in __init__
    self.__train_init()
  File "C:\Users\prana\Documents\Python_Stuff\Ariel-Data-Challenge-2024\trainer.py", line 75, in __train_init
    running_loss = self.__train_core()
                   ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\prana\Documents\Python_Stuff\Ariel-Data-Challenge-2024\trainer.py", line 119, in __train_core
    loss = self.__run_batch(inps, label)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\prana\Documents\Python_Stuff\Ariel-Data-Challenge-2024\trainer.py", line 133, in __run_batch
    preds = self.model(inps)
            ^^^^^^^^^^^^^^^^
  File "C:\Users\prana\Documents\Python_Stuff\Ariel-Data-Challenge-2024\.venv\Lib\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\prana\Documents\Python_Stuff\Ariel-Data-Challenge-2024\.venv\Lib\site-packages\torch\nn\modules\module.py", line 1603, in _call_impl
    result = forward_call(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\prana\Documents\Python_Stuff\Ariel-Data-Challenge-2024\models\BasicCNN.py", line 80, in forward
    x = self.cnn_block_1(x)
        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\prana\Documents\Python_Stuff\Ariel-Data-Challenge-2024\.venv\Lib\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\prana\Documents\Python_Stuff\Ariel-Data-Challenge-2024\.venv\Lib\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\prana\Documents\Python_Stuff\Ariel-Data-Challenge-2024\.venv\Lib\site-packages\torch\nn\modules\container.py", line 219, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "C:\Users\prana\Documents\Python_Stuff\Ariel-Data-Challenge-2024\.venv\Lib\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\prana\Documents\Python_Stuff\Ariel-Data-Challenge-2024\.venv\Lib\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\prana\Documents\Python_Stuff\Ariel-Data-Challenge-2024\.venv\Lib\site-packages\torch\nn\modules\conv.py", line 308, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\prana\Documents\Python_Stuff\Ariel-Data-Challenge-2024\.venv\Lib\site-packages\torch\nn\modules\conv.py", line 304, in _conv_forward
    return F.conv1d(input, weight, bias, self.stride,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Given groups=1, weight of size [512, 283, 3], expected input[283, 187, 32] to have 283 channels, but got 187 channels instead
